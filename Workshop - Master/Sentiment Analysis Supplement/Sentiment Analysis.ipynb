{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d96ae3c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run these code for first time PYthon users, or if you have never run libraries before.\n",
    "#If you have used these libraries before, ignore this line\n",
    "!pip install vaderSentiment\n",
    "!pip install nltk\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a46eef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "319c3d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace the csv_file_path with the computer path to the folder where the article files are located.\n",
    "#In given code, the articles are directly in Downloads folder of the computer.\n",
    "csv_file_path = '/Users/Admin/Downloads/'\n",
    "\n",
    "df_1920 = pd.read_csv(csv_file_path + \"articles1920.csv\", delimiter=';')\n",
    "df_2122 = pd.read_csv(csv_file_path + \"articles2122.csv\", delimiter=';')\n",
    "df_23 = pd.read_csv(csv_file_path + \"articles23.csv\", delimiter=';', encoding='latin1')\n",
    "df = pd.concat([df_1920, df_2122, df_23], ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "574ddc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove txt in FileNames\n",
    "df['FileNames'] = df['FileNames'].str.replace('txt', '')\n",
    "\n",
    "#Function to adjust mismatch in how article months are labelled (Jan 20XX -> 20xx_Jan)\n",
    "def convert_date_format(date_str):\n",
    "    if re.match(r'[a-zA-Z]+ \\d{4}', date_str):\n",
    "        month, year = date_str.split()\n",
    "        month_mapping = {'Jan': '1', 'Feb': '2', 'Mar': '3', 'Apr': '4', 'May': '5', 'Jun': '6',\n",
    "                         'Jul': '7', 'Aug': '8', 'Sep': '9', 'Oct': '10', 'Nov': '11', 'Dec': '12'}\n",
    "        month_numeric = month_mapping[month]\n",
    "        return f'{year}_{month_numeric}'\n",
    "    else:\n",
    "        return date_str\n",
    "    \n",
    "df['DirectoryName'] = df['DirectoryName'].apply(convert_date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e1f147a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_vader_sentiment(text):\n",
    "    vs = analyzer.polarity_scores(text)\n",
    "    \n",
    "    # Return the compound score\n",
    "    return vs['compound']\n",
    "\n",
    "#Get text sentiment (since titles are simple enough to apply VADER directly)\n",
    "df['Title_Sentiment'] = df['FileNames'].apply(get_vader_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "42cc571b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords and wordnet data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Get the English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#Remove English stop words from body text\n",
    "def preprocess_text(text, stop_words):\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    \n",
    "    cleaned_words = []\n",
    "    for word in words:\n",
    "        word = re.sub(r'\\W', '', word) \n",
    "        if word and word not in stop_words:\n",
    "            cleaned_words.append(word)\n",
    "            \n",
    "    cleaned_text = ' '.join(cleaned_words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "#Remove potential URLs in body text\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "df['Cleaned_Text'] = df['Content'].apply(remove_urls)\n",
    "df['Final_Text'] = df['Cleaned_Text'].apply(lambda x: preprocess_text(x, stop_words))\n",
    "df['Text_Sentiment'] = df['Final_Text'].apply(get_vader_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e5c8941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect aggregated sentiment (text & body), then make them dataframes\n",
    "\n",
    "Title = df.groupby('DirectoryName')['Title_Sentiment'].mean()\n",
    "Body = df.groupby('DirectoryName')['Text_Sentiment'].mean()\n",
    "\n",
    "Title = Title.reset_index()\n",
    "Title.columns = ['DirectoryName', 'Title_Sentiment']\n",
    "\n",
    "Body = Body.reset_index()\n",
    "Body.columns = ['DirectoryName', 'Body_Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5273e171",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Title text sentiment score\n",
    "Title['DirectoryName'] = pd.to_datetime(Title['DirectoryName'], format='%Y_%m').dt.strftime('%Y_%m')\n",
    "Title = Title.sort_values('DirectoryName')\n",
    "\n",
    "#Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x='DirectoryName', y='Title_Sentiment', data=Title, marker=None)\n",
    "plt.xticks(rotation=90, ha='right')\n",
    "plt.title('Sentiment Trend - Title Sentiment')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Aggregated Sentiment')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#For some reason seaborn shows a lot of warnings, just run the code twice so warnings don't show again\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8c1e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Body text sentiment score\n",
    "Body['DirectoryName'] = pd.to_datetime(Body['DirectoryName'], format='%Y_%m').dt.strftime('%Y_%m')\n",
    "Body = Body.sort_values('DirectoryName')\n",
    "\n",
    "#Visualization text sentiment\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x='DirectoryName', y='Body_Sentiment', data=Body, marker=None)\n",
    "plt.xticks(rotation=90, ha='right')\n",
    "plt.title('Sentiment Trend - Body Sentiment')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Aggregated Sentiment')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af07631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Title.to_csv('Title.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a9df13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Body.to_csv('Body.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
